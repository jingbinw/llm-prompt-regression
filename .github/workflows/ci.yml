name: CI/CD Pipeline

on:
  push:
   # branches: [ feature ]
  pull_request:
   # branches: [ main ]
  #schedule:
    # Run regression tests daily at 2 AM UTC
    #- cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of test to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - unit
        - integration
        - regression
      models:
        description: 'Models to test (comma-separated)'
        required: false
        default: 'gpt-3.5-turbo,gpt-4'

env:
  PYTHON_VERSION: '3.11'
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

jobs:
  # Code quality and linting
  lint:
    name: Code Quality
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy
        pip install -r requirements.txt
        
    - name: Lint with flake8
      run: |
        flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
        
    - name: Check code formatting with black
      run: black --check src/ tests/
      
    - name: Check import sorting with isort
      run: isort --check-only src/ tests/
      
    - name: Type checking with mypy
      run: mypy src/ --ignore-missing-imports

  # Unit tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html
        
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # Integration tests with Docker
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: lint
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build Docker image
      run: docker build -t llm-prompt-regression:test .
      
    - name: Run integration tests in Docker
      run: |
        docker run --rm \
          -e OPENAI_API_KEY="${{ env.OPENAI_API_KEY }}" \
          -e CI_MODE=true \
          -v $(pwd)/reports:/app/reports \
          llm-prompt-regression:test \
          pytest tests/integration/ -v

  # LLM Regression Tests
  regression-tests:
    name: LLM Regression Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'regression'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run LLM regression tests
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        DEFAULT_MODEL_1: ${{ github.event.inputs.models && fromJSON(format('["{0}"]', github.event.inputs.models))[0] || 'gpt-3.5-turbo' }}
        DEFAULT_MODEL_2: ${{ github.event.inputs.models && fromJSON(format('["{0}"]', github.event.inputs.models))[1] || 'gpt-4' }}
      run: |
        python -c "
        import asyncio
        import os
        from src.core.test_runner import TestRunner
        from src.utils.config_loader import ConfigLoader
        from src.core.report_generator import ReportGenerator
        
        async def run_tests():
            config_loader = ConfigLoader()
            config = config_loader.create_default_config()
            
            runner = TestRunner(os.getenv('OPENAI_API_KEY'))
            result = await runner.run_test(config)
            
            # Generate report
            report_gen = ReportGenerator()
            report = report_gen.generate_drift_report(
                TestSuiteResult(
                    suite_name='CI Regression Test',
                    start_time=result.start_time,
                    end_time=result.end_time,
                    test_results=[result],
                    overall_status=result.status
                )
            )
            
            # Generate HTML report
            html_file = report_gen.generate_html_report(report)
            print(f'HTML Report generated: {html_file}')
            
            # Check if drift was detected
            if report.drift_summary['drift_rate'] > 0.3:
                print('WARNING: High drift rate detected!')
                exit(1)
            else:
                print('Regression tests passed!')
        
        asyncio.run(run_tests())
        "
        
    - name: Upload test reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-reports-${{ github.run_number }}
        path: reports/
        
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Read report files if they exist
          const reportsDir = './reports';
          let comment = '## LLM Regression Test Results\n\n';
          
          try {
            if (fs.existsSync(reportsDir)) {
              const files = fs.readdirSync(reportsDir);
              const htmlFiles = files.filter(f => f.endsWith('.html'));
              
              if (htmlFiles.length > 0) {
                comment += `**Reports Generated:** ${htmlFiles.length} report(s)\n\n`;
                comment += '### Available Reports:\n';
                htmlFiles.forEach(file => {
                  comment += `- \`${file}\`\n`;
                });
              } else {
                comment += 'No HTML reports found\n';
              }
            } else {
              comment += 'Reports directory not found\n';
            }
          } catch (error) {
            comment += `Error reading reports: ${error.message}\n`;
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  # Security scan
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: lint
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
        
    - name: Run safety check
      run: safety check -r requirements.txt
      
    - name: Run bandit security scan
      run: bandit -r src/ -f json -o bandit-report.json || true
      
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-scan-results
        path: bandit-report.json

  # Notify on failure
  notify:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, regression-tests]
    if: failure()
    
    steps:
    - name: Notify failure
      run: |
        echo "One or more CI/CD steps failed!"
        echo "Check the logs for details."
